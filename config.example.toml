# otlp2parquet Configuration File Example
# ==========================================
# Copy this file to config.toml or .otlp2parquet.toml and customize for your environment
# Alternatively, use environment variables with the OTLP2PARQUET_ prefix (see config.example.env)
#
# Configuration Priority (highest to lowest):
#   1. Environment variables (OTLP2PARQUET_*)
#   2. This TOML file (config.toml or .otlp2parquet.toml)
#   3. Platform-specific defaults (Server, Lambda, Cloudflare Workers)
#
# Quick Start:
#   1. Copy this file: cp config.example.toml config.toml
#   2. Uncomment and customize sections for your platform
#   3. Start server: cargo run
#
# Platform Detection:
#   - Server (default): Neither AWS_LAMBDA_FUNCTION_NAME nor CF_WORKER present
#   - Lambda: AWS_LAMBDA_FUNCTION_NAME environment variable present (automatic)
#   - Cloudflare Workers: CF_WORKER environment variable present (automatic)

# ==============================================================================
# Batch Configuration (All Platforms)
# ==============================================================================
# Controls in-memory batching to create optimal Parquet file sizes.
# Larger batches = fewer files = better query performance & lower storage costs.
#
# Important:
#   - Server: Batching enabled by default (recommended for continuous ingestion)
#   - Lambda: Batching disabled by default (event-driven, process per request)
#   - Workers: Batching supported via Durable Objects with SQLite storage
#              Set enabled=true to use DO-based batching (requires DO bindings in wrangler.toml)
#
# Trade-offs:
#   - Larger batches = more memory usage, longer flush intervals
#   - Smaller batches = more files, higher storage costs, slower queries
[batch]
# Maximum number of rows per batch before flushing to storage
# Trigger: Flush occurs when batch reaches this row count
# Recommended: 10,000 - 1,000,000 rows depending on schema size
# Platform defaults: Server=200,000, Lambda=N/A (batching disabled), Workers=N/A
max_rows = 200_000

# Maximum bytes per batch before flushing (128 MB default)
# Trigger: Flush occurs when batch size exceeds this limit
# Recommended: 64 MB - 512 MB depending on available memory
# Platform defaults: Server=128MB, Lambda=N/A, Workers=N/A
max_bytes = 134_217_728  # 128 MB

# Maximum age of batch in seconds before flushing (time-based trigger)
# Trigger: Flush occurs after this duration, even if batch is small
# Recommended: 5-60 seconds depending on latency requirements
# Purpose: Ensures data is not buffered indefinitely during low-traffic periods
# Platform defaults: Server=10s, Lambda=N/A, Workers=N/A
max_age_secs = 10

# Enable or disable batching entirely
# true = Batch data in memory before writing (recommended for server)
# false = Write immediately per request (required for Lambda/Workers)
# Platform defaults: Server=true, Lambda=false, Workers=false
enabled = true


# ==============================================================================
# Cloudflare Workers Batching (with Durable Objects)
# ==============================================================================
# When running on Cloudflare Workers with Durable Objects enabled, batching
# uses memory-only buffering within the Durable Object.
#
# Architecture:
#   1. Worker receives OTLP request, parses and converts to Arrow
#   2. Arrow batches routed to per-service Durable Objects
#   3. DO accumulates batches in memory (data loss on eviction is acceptable for telemetry)
#   4. Alarms trigger periodic flush to R2 when thresholds exceeded
#
# To enable:
#   1. Set batch.enabled = true in config
#   2. Configure wrangler.toml with DO bindings:
#      [[durable_objects.bindings]]
#      name = "BATCHER"
#      class_name = "OtlpBatcher"
#
# Environment variables for Workers batching (set in wrangler.toml [vars]):
#   OTLP2PARQUET_BATCH_MAX_ROWS = 50000      # Flush at 50k records
#   OTLP2PARQUET_BATCH_MAX_BYTES = 10485760  # Flush at 10MB
#   OTLP2PARQUET_BATCH_MAX_AGE_SECS = 60     # Flush after 60 seconds
#
# Recommended Workers batch settings (lower than Server due to DO constraints):
# [batch]
# enabled = true
# max_rows = 50_000       # Flush at 50k records (vs 200k for Server)
# max_bytes = 10_485_760  # Flush at 10MB (vs 128MB for Server)
# max_age_secs = 60       # Flush after 60 seconds (vs 10s for Server)


# ==============================================================================
# Request Handling Configuration (All Platforms)
# ==============================================================================
# Controls HTTP request validation and size limits
[request]
# Maximum HTTP payload size in bytes
# Purpose: Prevents out-of-memory errors from extremely large requests
# Recommendation: Set based on available memory and expected batch sizes
#   - Server: 8-100 MB (plenty of memory available)
#   - Lambda: 6 MB max (AWS Lambda payload limit)
#   - Workers: 1-10 MB (Cloudflare Workers 128 MB memory limit)
# Platform defaults: Server=8MB, Lambda=6MB, Workers=10MB
max_payload_bytes = 8_388_608  # 8 MB


# ==============================================================================
# Storage Configuration
# ==============================================================================
# Configures where Parquet files are written
#
# Supported backends:
#   - "fs": Local filesystem (development, testing)
#   - "s3": AWS S3 or S3-compatible storage (production)
#   - "r2": Cloudflare R2 (Cloudflare Workers)
#
# Platform defaults (auto-detected):
#   - Server (default): "fs"
#   - Lambda: "s3" (required, event-driven constraint)
#   - Cloudflare Workers: "r2" (required, WASM constraint)
[storage]
# Storage backend type
# Options: "fs" | "s3" | "r2"
backend = "fs"

# Parquet row group size (advanced tuning)
# Controls internal Parquet file structure for optimal compression and query performance
# Recommended: 32,768 - 1,048,576 rows per group
# Default: 32,768 rows
# Larger values = better compression, slower random access
# parquet_row_group_size = 32768

# --- Filesystem Storage (backend="fs") ---
# Best for: Local development, testing, single-machine deployments
# Limitations: No distributed access, not suitable for serverless
[storage.fs]
# Local directory path for storing Parquet files
# Path structure (Hive-style partitioning):
#   logs/{service}/year={year}/month={month}/day={day}/hour={hour}/file.parquet
# Example: ./data/logs/my-service/year=2025/month=01/day=15/hour=10/abc123.parquet
path = "./data"

# --- S3 Storage (backend="s3") ---
# Best for: Production deployments, AWS ecosystem integration
# Supports: AWS S3, MinIO, LocalStack, any S3-compatible storage
# [storage.s3]
# # Required: S3 bucket name
# bucket = "my-otlp-bucket"
#
# # Required: AWS region
# region = "us-east-1"
#
# # Optional: Custom S3 endpoint for S3-compatible services
# # Leave unset for standard AWS S3
# # Examples:
# #   - MinIO: "http://localhost:9000"
# #   - LocalStack: "http://localhost:4566"
# #   - DigitalOcean Spaces: "https://nyc3.digitaloceanspaces.com"
# # endpoint = "http://localhost:9000"
#
# # Credentials: Auto-discovered from environment or IAM role
# # Priority: AWS_ACCESS_KEY_ID/AWS_SECRET_ACCESS_KEY env vars > IAM role
# # For Lambda: Automatically uses Lambda execution role (no explicit credentials needed)

# --- R2 Storage (backend="r2") ---
# Best for: Cloudflare Workers, edge deployments, cost-effective storage
# Requirements: Cloudflare account with R2 enabled
# [storage.r2]
# # Required: R2 bucket name
# bucket = "my-r2-bucket"
#
# # Required: Cloudflare account ID
# # Find at: Cloudflare Dashboard → R2 → Overview
# account_id = "your_account_id"
#
# # Required: R2 API credentials (uses AWS S3 API compatibility)
# # Generate at: Cloudflare Dashboard → R2 → Manage R2 API Tokens
# access_key_id = "your_r2_access_key"
# secret_access_key = "your_r2_secret_key"
#
# # Optional: Custom R2 endpoint (auto-generated if not specified)
# # Default: https://{account_id}.r2.cloudflarestorage.com
# # endpoint = "https://your_account_id.r2.cloudflarestorage.com"


# ==============================================================================
# Server-Specific Configuration (Server Platform Only)
# ==============================================================================
# These settings only apply when running as a long-lived HTTP server
# Not used for Lambda or Cloudflare Workers deployments
[server]
# HTTP server listen address
# Format: "host:port"
# Use 0.0.0.0 to listen on all interfaces (recommended for containers)
# Use 127.0.0.1 to listen only on localhost (more secure for local dev)
# Default port: 4318 (OTLP HTTP standard port)
listen_addr = "0.0.0.0:4318"

# Log level: Controls verbosity of application logs
# Options: "trace" | "debug" | "info" | "warn" | "error"
#   - trace: Very verbose, includes all internal operations (debugging only)
#   - debug: Verbose, includes request/response details
#   - info: Normal operational messages (recommended for production)
#   - warn: Only warnings and errors
#   - error: Only errors
# Default: "info"
log_level = "info"

# Log format: Output format for logs
# Options: "text" | "json"
#   - text: Human-readable, colorized output (best for local development)
#   - json: Structured JSON output (best for log aggregation services)
# Default: "text"
# Recommendation: Use "json" for production/container environments
log_format = "text"


# ==============================================================================
# Lambda-Specific Configuration (Lambda Platform Only)
# ==============================================================================
[lambda]
# (Currently no lambda-specific config - uses common settings above)


# ==============================================================================
# Cloudflare Workers-Specific Configuration (Cloudflare Platform Only)
# ==============================================================================
# [cloudflare]
# (Currently no cloudflare-specific config - uses common settings above)
