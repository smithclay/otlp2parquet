//! Apache Iceberg catalog integration for otlp2parquet.
//!
//! This crate provides generic Iceberg REST catalog integration for committing
//! Parquet files to Iceberg tables. Works with any Iceberg REST catalog including
//! AWS S3 Tables, AWS Glue, Tabular, Polaris, and others.
//!
//! # Features
//!
//! - Generic REST catalog support (not vendor-specific)
//! - Transaction-based commits with ACID guarantees
//! - Arrow to Iceberg schema conversion with field ID preservation
//! - DataFile metadata extraction from Parquet statistics
//! - Warn-and-succeed error handling (catalog failures don't block ingestion)
//!
//! # Architecture
//!
//! This crate is **only** used by Lambda and Server runtimes, not Cloudflare Workers.
//! This keeps the WASM binary size minimal.
//!
//! ```text
//! otlp2parquet-storage (Parquet writing)
//!          ↓
//! otlp2parquet-iceberg (Catalog commits)
//!          ↓
//!     ┌────┴────┐
//!     ↓         ↓
//!  lambda     server
//! ```
//!
//! # Configuration
//!
//! Set environment variables with the `OTLP2PARQUET_ICEBERG_` prefix:
//!
//! ```bash
//! # Required
//! OTLP2PARQUET_ICEBERG_REST_URI="https://s3tables.us-east-1.amazonaws.com/iceberg"
//!
//! # Optional - Table names (defaults shown)
//! OTLP2PARQUET_ICEBERG_TABLE_LOGS="otel_logs"
//! OTLP2PARQUET_ICEBERG_TABLE_TRACES="otel_traces"
//! OTLP2PARQUET_ICEBERG_TABLE_METRICS_GAUGE="otel_metrics_gauge"
//! OTLP2PARQUET_ICEBERG_TABLE_METRICS_SUM="otel_metrics_sum"
//! OTLP2PARQUET_ICEBERG_TABLE_METRICS_HISTOGRAM="otel_metrics_histogram"
//! OTLP2PARQUET_ICEBERG_TABLE_METRICS_EXPONENTIAL_HISTOGRAM="otel_metrics_exponential_histogram"
//! OTLP2PARQUET_ICEBERG_TABLE_METRICS_SUMMARY="otel_metrics_summary"
//!
//! # Optional - Catalog settings
//! OTLP2PARQUET_ICEBERG_NAMESPACE="otel.production"
//! OTLP2PARQUET_ICEBERG_WAREHOUSE="s3://my-warehouse"
//! ```
//!
//! # Example Usage
//!
//! ```no_run
//! use otlp2parquet_storage::iceberg::init::{initialize_committer, InitResult};
//!
//! # async fn example() -> anyhow::Result<()> {
//! // Initialize committer from environment variables
//! let committer = match initialize_committer().await {
//!     InitResult::Success { committer, table_count } => {
//!         println!("Initialized with {} tables", table_count);
//!         committer
//!     }
//!     InitResult::NotConfigured(msg) => {
//!         println!("Not configured: {}", msg);
//!         return Ok(());
//!     }
//!     InitResult::CatalogError(msg) | InitResult::NamespaceError(msg) => {
//!         println!("Init failed: {}", msg);
//!         return Ok(());
//!     }
//! };
//!
//! // Commit Parquet files (after writing to storage)
//! // committer.commit_with_signal("logs", None, &parquet_results).await?;
//! # Ok(())
//! # }
//! ```
//!
//! # Error Handling
//!
//! The integration uses a **warn-and-succeed** pattern:
//! - If not configured, runtimes continue without catalog integration
//! - If catalog operations fail, errors are logged as warnings
//! - Parquet files are always written to storage regardless of catalog status
//!
//! This ensures ingestion is never blocked by catalog availability.

// Re-export main types
pub use catalog::IcebergCatalog;
pub use writer::{IcebergWriter, IcebergConfig};

// Module declarations
pub mod arrow_convert;
pub mod catalog;
pub mod datafile_convert;
pub mod http;
#[cfg(not(target_arch = "wasm32"))]
pub mod init;
pub mod protocol;
pub mod types;
pub mod validation;
pub mod writer;

use std::collections::HashMap;
use std::env;
use std::sync::Arc;

use anyhow::{Context, Result as AnyhowResult};
use crate::ParquetWriteResult;
use serde::{Deserialize, Serialize};
use tracing::{debug, info, instrument, warn};

/// Configuration for Iceberg REST catalog integration.
///
/// Works with any Iceberg REST catalog including:
/// - AWS S3 Tables (via Iceberg REST endpoint)
/// - AWS Glue Data Catalog
/// - Tabular
/// - Other REST-compatible catalogs
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq)]
pub struct IcebergRestConfig {
    /// REST catalog endpoint URI
    /// Examples:
    /// - S3 Tables: `https://s3tables.<region>.amazonaws.com/iceberg`
    /// - Glue: `https://glue.<region>.amazonaws.com/iceberg`
    pub rest_uri: String,
    /// Warehouse location (optional, depends on catalog)
    pub warehouse: Option<String>,
    /// Namespace for the table (dot-separated, e.g., "otel.logs")
    pub namespace: Vec<String>,
    /// Table names per signal type (key: "logs", "traces", "metrics:gauge", etc.)
    pub tables: HashMap<String, String>,
    /// Catalog name (defaults to "rest")
    #[serde(default = "default_catalog_name")]
    pub catalog_name: String,
    #[serde(default = "default_format_version")]
    pub format_version: i32,
    #[serde(default = "default_target_file_size_bytes")]
    pub target_file_size_bytes: u64,
    #[serde(default = "default_staging_prefix")]
    pub staging_prefix: String,
}

const DEFAULT_CATALOG_NAME: &str = "rest";
const DEFAULT_STAGING_PREFIX: &str = "data/incoming";
const DEFAULT_TARGET_FILE_SIZE: u64 = 512 * 1024 * 1024;

fn default_catalog_name() -> String {
    DEFAULT_CATALOG_NAME.to_string()
}

fn default_format_version() -> i32 {
    2
}

fn default_target_file_size_bytes() -> u64 {
    DEFAULT_TARGET_FILE_SIZE
}

fn default_staging_prefix() -> String {
    DEFAULT_STAGING_PREFIX.to_string()
}

impl Default for IcebergRestConfig {
    fn default() -> Self {
        Self {
            rest_uri: String::new(),
            warehouse: None,
            namespace: Vec::new(),
            tables: HashMap::new(),
            catalog_name: default_catalog_name(),
            format_version: default_format_version(),
            target_file_size_bytes: default_target_file_size_bytes(),
            staging_prefix: default_staging_prefix(),
        }
    }
}

impl IcebergRestConfig {
    /// Load configuration from environment variables.
    ///
    /// Required:
    /// - `OTLP2PARQUET_ICEBERG_REST_URI`: REST catalog endpoint
    ///
    /// Optional:
    /// - `OTLP2PARQUET_ICEBERG_WAREHOUSE`: Warehouse location
    /// - `OTLP2PARQUET_ICEBERG_NAMESPACE`: Dot-separated namespace (e.g., "otel.logs")
    /// - `OTLP2PARQUET_ICEBERG_CATALOG_NAME`: Catalog name (default: "rest")
    /// - `OTLP2PARQUET_ICEBERG_STAGING_PREFIX`: Staging prefix for data files
    /// - `OTLP2PARQUET_ICEBERG_TARGET_FILE_SIZE_BYTES`: Target file size
    /// - Table names (with defaults):
    ///   - `OTLP2PARQUET_ICEBERG_TABLE_LOGS` (default: "otel_logs")
    ///   - `OTLP2PARQUET_ICEBERG_TABLE_TRACES` (default: "otel_traces")
    ///   - `OTLP2PARQUET_ICEBERG_TABLE_METRICS_GAUGE` (default: "otel_metrics_gauge")
    ///   - `OTLP2PARQUET_ICEBERG_TABLE_METRICS_SUM` (default: "otel_metrics_sum")
    ///   - `OTLP2PARQUET_ICEBERG_TABLE_METRICS_HISTOGRAM` (default: "otel_metrics_histogram")
    ///   - `OTLP2PARQUET_ICEBERG_TABLE_METRICS_EXPONENTIAL_HISTOGRAM` (default: "otel_metrics_exponential_histogram")
    ///   - `OTLP2PARQUET_ICEBERG_TABLE_METRICS_SUMMARY` (default: "otel_metrics_summary")
    pub fn from_env() -> AnyhowResult<Self> {
        let rest_uri = env::var("OTLP2PARQUET_ICEBERG_REST_URI").context(
            "OTLP2PARQUET_ICEBERG_REST_URI must be set (e.g., https://s3tables.us-east-1.amazonaws.com/iceberg)",
        )?;

        let warehouse = env::var("OTLP2PARQUET_ICEBERG_WAREHOUSE").ok();

        let namespace = env::var("OTLP2PARQUET_ICEBERG_NAMESPACE")
            .unwrap_or_default()
            .split('.')
            .filter(|segment| !segment.trim().is_empty())
            .map(|segment| segment.trim().to_string())
            .collect::<Vec<_>>();

        // Build table map with defaults for each signal type
        let mut tables = HashMap::new();

        tables.insert(
            "logs".to_string(),
            env::var("OTLP2PARQUET_ICEBERG_TABLE_LOGS").unwrap_or_else(|_| "otel_logs".to_string()),
        );

        tables.insert(
            "traces".to_string(),
            env::var("OTLP2PARQUET_ICEBERG_TABLE_TRACES")
                .unwrap_or_else(|_| "otel_traces".to_string()),
        );

        tables.insert(
            "metrics:gauge".to_string(),
            env::var("OTLP2PARQUET_ICEBERG_TABLE_METRICS_GAUGE")
                .unwrap_or_else(|_| "otel_metrics_gauge".to_string()),
        );

        tables.insert(
            "metrics:sum".to_string(),
            env::var("OTLP2PARQUET_ICEBERG_TABLE_METRICS_SUM")
                .unwrap_or_else(|_| "otel_metrics_sum".to_string()),
        );

        tables.insert(
            "metrics:histogram".to_string(),
            env::var("OTLP2PARQUET_ICEBERG_TABLE_METRICS_HISTOGRAM")
                .unwrap_or_else(|_| "otel_metrics_histogram".to_string()),
        );

        tables.insert(
            "metrics:exponential_histogram".to_string(),
            env::var("OTLP2PARQUET_ICEBERG_TABLE_METRICS_EXPONENTIAL_HISTOGRAM")
                .unwrap_or_else(|_| "otel_metrics_exponential_histogram".to_string()),
        );

        tables.insert(
            "metrics:summary".to_string(),
            env::var("OTLP2PARQUET_ICEBERG_TABLE_METRICS_SUMMARY")
                .unwrap_or_else(|_| "otel_metrics_summary".to_string()),
        );

        let catalog_name = env::var("OTLP2PARQUET_ICEBERG_CATALOG_NAME")
            .unwrap_or_else(|_| DEFAULT_CATALOG_NAME.to_string());

        let staging_prefix = env::var("OTLP2PARQUET_ICEBERG_STAGING_PREFIX")
            .unwrap_or_else(|_| DEFAULT_STAGING_PREFIX.to_string());

        let target_file_size_bytes = env::var("OTLP2PARQUET_ICEBERG_TARGET_FILE_SIZE_BYTES")
            .ok()
            .and_then(|raw| raw.parse::<u64>().ok())
            .unwrap_or(DEFAULT_TARGET_FILE_SIZE);

        Ok(Self {
            rest_uri,
            warehouse,
            namespace,
            tables,
            catalog_name,
            format_version: default_format_version(),
            target_file_size_bytes,
            staging_prefix,
        })
    }
}

/// Committer that appends Parquet files to an Iceberg table.
///
/// Wrapper around the minimal IcebergCatalog implementation that maintains
/// backward compatibility with existing Lambda/Server code.
///
/// This:
/// 1. Converts ParquetWriteResult to DataFile descriptors
/// 2. Delegates to IcebergCatalog.commit_with_signal
/// 3. Implements warn-and-succeed pattern
#[cfg(not(target_arch = "wasm32"))]
pub struct IcebergCommitter {
    catalog: Arc<IcebergCatalog<http::ReqwestHttpClient>>,
}

#[cfg(not(target_arch = "wasm32"))]
impl IcebergCommitter {
    pub fn new(catalog: Arc<IcebergCatalog<http::ReqwestHttpClient>>) -> Self {
        Self { catalog }
    }

    /// Commit Parquet files to the appropriate Iceberg table based on signal type.
    ///
    /// This:
    /// 1. Delegates to catalog's commit_with_signal method (which handles schema loading and DataFile conversion internally)
    /// 2. Implements warn-and-succeed: errors logged but return Ok(())
    ///
    /// # Parameters
    /// - `signal_type`: "logs", "traces", or "metrics"
    /// - `metric_type`: For metrics only - "gauge", "sum", "histogram", "exponential_histogram", or "summary"
    /// - `parquet_results`: Parquet files to commit
    ///
    /// # Note
    /// The catalog's commit_with_signal now expects `Vec<DataFile>`, but we pass `ParquetWriteResult`.
    /// We need to add a conversion method that takes `ParquetWriteResult` and converts to `DataFile`.
    /// For now, we'll build DataFiles using the Arrow schema from the results.
    #[cfg_attr(not(target_arch = "wasm32"), instrument(skip(self, parquet_results), fields(num_files = parquet_results.len(), signal_type, metric_type))]
    pub async fn commit_with_signal(
        &self,
        signal_type: &str,
        metric_type: Option<&str>,
        parquet_results: &[ParquetWriteResult],
    ) -> AnyhowResult<()> {
        if parquet_results.is_empty() {
            debug!("No files to commit");
            return Ok(());
        }

        // Convert ParquetWriteResult to Arrow schema, then to Iceberg schema, then to DataFile
        // This is a simplified approach that doesn't require loading the table first
        let mut data_files = Vec::with_capacity(parquet_results.len());
        for result in parquet_results {
            // Convert Arrow schema from Parquet to Iceberg schema
            let arrow_schema = &result.arrow_schema;
            let iceberg_schema = match arrow_convert::arrow_to_iceberg_schema(arrow_schema) {
                Ok(schema) => schema,
                Err(e) => {
                    warn!(
                        path = %result.path,
                        error = %e,
                        "failed to convert Arrow schema to Iceberg - skipping file"
                    );
                    continue;
                }
            };

            // Build DataFile from Parquet metadata
            let data_file = match datafile_convert::build_data_file(result, &iceberg_schema) {
                Ok(df) => df,
                Err(e) => {
                    warn!(
                        path = %result.path,
                        error = %e,
                        "failed to build DataFile from Parquet metadata - skipping file"
                    );
                    continue;
                }
            };
            data_files.push(data_file);
        }

        if data_files.is_empty() {
            warn!("No valid DataFiles to commit after conversion");
            return Ok(());
        }

        let total_rows: u64 = data_files.iter().map(|f| f.record_count).sum();
        let total_bytes: u64 = data_files.iter().map(|f| f.file_size_in_bytes).sum();

        debug!(
            signal_type = signal_type,
            metric_type = ?metric_type,
            num_files = data_files.len(),
            total_rows = total_rows,
            total_bytes = total_bytes,
            "committing files to Iceberg table"
        );

        // Delegate to catalog's commit_with_signal
        if let Err(e) = self
            .catalog
            .commit_with_signal(signal_type, metric_type, data_files)
            .await
        {
            warn!(
                signal_type = signal_type,
                metric_type = ?metric_type,
                error = %e,
                "failed to commit to catalog - files written to storage but not cataloged"
            );
            return Ok(()); // Warn and succeed
        }

        info!(
            signal_type = signal_type,
            metric_type = ?metric_type,
            num_files = parquet_results.len(),
            total_rows = total_rows,
            total_bytes = total_bytes,
            "successfully committed files to Iceberg table"
        );

        Ok(())
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::sync::Mutex;

    // Mutex to serialize tests that modify environment variables
    static ENV_MUTEX: Mutex<()> = Mutex::new(());

    #[test]
    fn test_config_from_env() {
        let _lock = ENV_MUTEX.lock().unwrap();

        // Set required environment variables
        std::env::set_var(
            "OTLP2PARQUET_ICEBERG_REST_URI",
            "https://test.example.com/iceberg",
        );
        std::env::set_var("OTLP2PARQUET_ICEBERG_NAMESPACE", "otel.logs");

        // Set custom table names for some signals
        std::env::set_var("OTLP2PARQUET_ICEBERG_TABLE_LOGS", "custom_logs_table");
        std::env::set_var("OTLP2PARQUET_ICEBERG_TABLE_TRACES", "custom_traces_table");

        let config = IcebergRestConfig::from_env().unwrap();

        assert_eq!(config.rest_uri, "https://test.example.com/iceberg");
        assert_eq!(config.namespace, vec!["otel", "logs"]);
        assert_eq!(config.catalog_name, DEFAULT_CATALOG_NAME);
        assert_eq!(config.staging_prefix, DEFAULT_STAGING_PREFIX);
        assert_eq!(config.target_file_size_bytes, DEFAULT_TARGET_FILE_SIZE);

        // Verify table map contains all 7 signal types
        assert_eq!(config.tables.len(), 7);
        assert_eq!(
            config.tables.get("logs"),
            Some(&"custom_logs_table".to_string())
        );
        assert_eq!(
            config.tables.get("traces"),
            Some(&"custom_traces_table".to_string())
        );
        assert_eq!(
            config.tables.get("metrics:gauge"),
            Some(&"otel_metrics_gauge".to_string())
        ); // default
        assert_eq!(
            config.tables.get("metrics:sum"),
            Some(&"otel_metrics_sum".to_string())
        ); // default
        assert_eq!(
            config.tables.get("metrics:histogram"),
            Some(&"otel_metrics_histogram".to_string())
        ); // default
        assert_eq!(
            config.tables.get("metrics:exponential_histogram"),
            Some(&"otel_metrics_exponential_histogram".to_string())
        ); // default
        assert_eq!(
            config.tables.get("metrics:summary"),
            Some(&"otel_metrics_summary".to_string())
        ); // default

        // Clean up
        std::env::remove_var("OTLP2PARQUET_ICEBERG_REST_URI");
        std::env::remove_var("OTLP2PARQUET_ICEBERG_NAMESPACE");
        std::env::remove_var("OTLP2PARQUET_ICEBERG_TABLE_LOGS");
        std::env::remove_var("OTLP2PARQUET_ICEBERG_TABLE_TRACES");
    }

    #[test]
    fn test_config_optional_warehouse() {
        let _lock = ENV_MUTEX.lock().unwrap();

        std::env::set_var(
            "OTLP2PARQUET_ICEBERG_REST_URI",
            "https://test.example.com/iceberg",
        );
        std::env::set_var("OTLP2PARQUET_ICEBERG_WAREHOUSE", "s3://my-bucket/warehouse");

        let config = IcebergRestConfig::from_env().unwrap();

        assert_eq!(
            config.warehouse,
            Some("s3://my-bucket/warehouse".to_string())
        );

        // Clean up
        std::env::remove_var("OTLP2PARQUET_ICEBERG_REST_URI");
        std::env::remove_var("OTLP2PARQUET_ICEBERG_WAREHOUSE");
    }

    #[test]
    fn test_namespace_empty_error() {
        // Verify that empty namespace is rejected
        use super::catalog::NamespaceIdent;

        let empty_namespace: Vec<String> = vec![];
        assert!(NamespaceIdent::from_vec(empty_namespace).is_err());
    }

    #[test]
    fn test_default_values() {
        let config = IcebergRestConfig::default();

        assert_eq!(config.catalog_name, DEFAULT_CATALOG_NAME);
        assert_eq!(config.format_version, 2);
        assert_eq!(config.target_file_size_bytes, DEFAULT_TARGET_FILE_SIZE);
        assert_eq!(config.staging_prefix, DEFAULT_STAGING_PREFIX);
    }
}
