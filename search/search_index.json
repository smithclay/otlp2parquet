{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"otlp2parquet","text":"<p>Store your observability data in cheap object storage. Servers optional.</p> <p>otlp2parquet ingests OpenTelemetry logs, traces, and metrics, converts them to Parquet, and writes to object storage. Run locally with Docker or on your own servers.</p>"},{"location":"#why-otlp2parquet","title":"Why otlp2parquet?","text":"<ul> <li>Cheap storage - Write to S3-compatible object storage or local filesystem. No vendor lock-in.</li> <li>Portable - Run as a local binary or container.</li> <li>Query anywhere - DuckDB, Spark, Trino - any Parquet reader works.</li> </ul>"},{"location":"#get-started","title":"Get Started","text":"<p>Deploy in under 5 minutes:</p> <pre><code># Install the CLI\ncargo install otlp2parquet\n\n# Run locally\notlp2parquet\n</code></pre> <p>Or run locally with Docker:</p> <pre><code>docker-compose up\ncurl -X POST http://localhost:4318/v1/logs \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"resourceLogs\":[{\"resource\":{},\"scopeLogs\":[{\"scope\":{},\"logRecords\":[{\"body\":{\"stringValue\":\"Hello\"}}]}]}]}'\n</code></pre> <p>Deploy Now \u2192</p>"},{"location":"deploying/","title":"Deploy","text":"<p>Run otlp2parquet locally or self-host it with Docker. The same server binary can write to local disk or S3-compatible object storage.</p>"},{"location":"deploying/#prerequisites","title":"Prerequisites","text":"<p>Install the CLI:</p> <pre><code>cargo install otlp2parquet\n</code></pre> <p>Or download from GitHub Releases.</p>"},{"location":"deploying/#docker-local-self-hosted","title":"Docker (Local / Self-Hosted)","text":"<p>Run locally for development or self-host in your infrastructure.</p>"},{"location":"deploying/#quick-start","title":"Quick Start","text":"<pre><code>docker-compose up\n</code></pre> <p>This starts: - otlp2parquet on port 4318 - MinIO (S3-compatible storage) on ports 9000/9001</p>"},{"location":"deploying/#send-test-data","title":"Send test data","text":"<pre><code>curl -X POST http://localhost:4318/v1/logs \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"resourceLogs\":[{\"resource\":{},\"scopeLogs\":[{\"scope\":{},\"logRecords\":[{\"body\":{\"stringValue\":\"Hello\"}}]}]}]}'\n</code></pre>"},{"location":"deploying/#verify-output","title":"Verify output","text":"<p>Open MinIO Console at <code>http://localhost:9001</code> (login: <code>minioadmin</code>/<code>minioadmin</code>).</p> Production with object storage <p>Point Docker at any S3-compatible storage: <pre><code>OTLP2PARQUET_STORAGE_BACKEND=s3 \\\nOTLP2PARQUET_S3_BUCKET=my-prod-bucket \\\nOTLP2PARQUET_S3_REGION=us-west-2 \\\nOTLP2PARQUET_S3_ENDPOINT=https://object.example.com \\\ndocker-compose up\n</code></pre></p> Common commands <ul> <li>View logs: <code>docker-compose logs -f otlp2parquet</code></li> <li>Reset data: <code>docker-compose down -v</code></li> <li>Rebuild: <code>docker-compose up --build</code></li> </ul>"},{"location":"deploying/#local-binary","title":"Local Binary","text":"<p>Run the server directly:</p> <pre><code>otlp2parquet --config config.toml\n</code></pre> <p>Use <code>config.example.toml</code> as a starting point and customize the storage section for your environment.</p>"},{"location":"querying/","title":"Query Data","text":"<p>Query your Parquet files with DuckDB or any Parquet reader.</p>"},{"location":"querying/#duckdb-recommended","title":"DuckDB (Recommended)","text":"<p>DuckDB reads Parquet files directly from disk or S3.</p>"},{"location":"querying/#local-files","title":"Local files","text":"<pre><code>SELECT Timestamp, ServiceName, Body\nFROM read_parquet('data/logs/**/*.parquet')\nORDER BY Timestamp DESC\nLIMIT 10;\n</code></pre>"},{"location":"querying/#s3-minio","title":"S3 / MinIO","text":"<pre><code>-- Configure S3 access\nINSTALL httpfs; LOAD httpfs;\nSET s3_region = 'us-west-2';\nSET s3_access_key_id = 'your-key';\nSET s3_secret_access_key = 'your-secret';\n\n-- For MinIO (local Docker setup)\nSET s3_endpoint = 'localhost:9000';\nSET s3_url_style = 'path';\nSET s3_use_ssl = false;\n\n-- Query logs\nSELECT Timestamp, ServiceName, Body\nFROM read_parquet('s3://otlp-logs/logs/**/*.parquet')\nWHERE Timestamp &gt; current_timestamp - interval '1 hour'\nLIMIT 100;\n</code></pre>"},{"location":"querying/#example-queries","title":"Example Queries","text":""},{"location":"querying/#logs-by-service","title":"Logs by service","text":"<pre><code>SELECT ServiceName, COUNT(*) as count\nFROM read_parquet('s3://bucket/logs/**/*.parquet')\nGROUP BY ServiceName\nORDER BY count DESC;\n</code></pre>"},{"location":"querying/#recent-errors","title":"Recent errors","text":"<pre><code>SELECT Timestamp, ServiceName, SeverityText, Body\nFROM read_parquet('s3://bucket/logs/**/*.parquet')\nWHERE SeverityText IN ('ERROR', 'FATAL')\n  AND Timestamp &gt; current_timestamp - interval '1 hour'\nORDER BY Timestamp DESC\nLIMIT 50;\n</code></pre>"},{"location":"querying/#error-traces","title":"Error traces","text":"<pre><code>SELECT TraceId, SpanName, Duration, StatusMessage\nFROM read_parquet('s3://bucket/traces/**/*.parquet')\nWHERE StatusCode = 'STATUS_CODE_ERROR'\nORDER BY Duration DESC\nLIMIT 20;\n</code></pre>"},{"location":"querying/#slow-traces","title":"Slow traces","text":"<pre><code>SELECT\n  TraceId,\n  SpanName,\n  Duration / 1e9 as duration_seconds,\n  ServiceName\nFROM read_parquet('s3://bucket/traces/**/*.parquet')\nWHERE Duration &gt; 5e9  -- 5 seconds in nanoseconds\nORDER BY Duration DESC\nLIMIT 10;\n</code></pre>"},{"location":"querying/#metrics-over-time","title":"Metrics over time","text":"<pre><code>SELECT\n  date_trunc('hour', Timestamp) as hour,\n  MetricName,\n  AVG(Value) as avg_value\nFROM read_parquet('s3://bucket/metrics/gauge/**/*.parquet')\nGROUP BY hour, MetricName\nORDER BY hour;\n</code></pre>"},{"location":"querying/#histogram-percentiles","title":"Histogram percentiles","text":"<pre><code>-- Approximate p95 from histogram buckets\nSELECT\n  MetricName,\n  ExplicitBounds,\n  BucketCounts\nFROM read_parquet('s3://bucket/metrics/histogram/**/*.parquet')\nWHERE MetricName = 'http.server.duration'\nLIMIT 10;\n</code></pre>"},{"location":"querying/#join-logs-and-traces","title":"Join logs and traces","text":"<pre><code>SELECT\n  l.Timestamp,\n  l.ServiceName,\n  l.Body,\n  t.SpanName,\n  t.Duration\nFROM read_parquet('s3://bucket/logs/**/*.parquet') l\nJOIN read_parquet('s3://bucket/traces/**/*.parquet') t\n  ON l.TraceId = t.TraceId\nWHERE l.SeverityText = 'ERROR'\nLIMIT 100;\n</code></pre>"},{"location":"querying/#other-query-engines","title":"Other Query Engines","text":"Apache Spark <pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"otlp\").getOrCreate()\n\n# Read logs\nlogs = spark.read.parquet(\"s3://my-bucket/logs/\")\nlogs.filter(logs.ServiceName == \"my-service\").show()\n\n# Read traces\ntraces = spark.read.parquet(\"s3://my-bucket/traces/\")\ntraces.filter(traces.StatusCode == \"STATUS_CODE_ERROR\") \\\n      .orderBy(\"Duration\", ascending=False) \\\n      .show(20)\n\n# Read gauge metrics\nmetrics = spark.read.parquet(\"s3://my-bucket/metrics/gauge/\")\nmetrics.groupBy(\"MetricName\").avg(\"Value\").show()\n\n# Join logs and traces\nerror_logs = logs.filter(logs.SeverityText == \"ERROR\")\nerror_traces = error_logs.join(traces, \"TraceId\")\nerror_traces.select(\"Timestamp\", \"ServiceName\", \"Body\", \"SpanName\", \"Duration\").show()\n</code></pre>"},{"location":"querying/#tips","title":"Tips","text":"<p>Partition pruning: Use time-based filters to skip scanning irrelevant files:</p> <pre><code>-- Good: scans only recent partitions\nWHERE Timestamp &gt; current_timestamp - interval '1 hour'\n\n-- Bad: scans all files\nWHERE ServiceName = 'api'\n</code></pre> <p>Column projection: Select only needed columns to reduce I/O:</p> <pre><code>-- Good: reads 3 columns\nSELECT ServiceName, Timestamp, Body FROM ...\n\n-- Bad: reads all columns\nSELECT * FROM ...\n</code></pre> <p>Predicate pushdown: Filter in the query, not after:</p> <pre><code>-- Good: filtering happens during scan\nWHERE SeverityText = 'ERROR'\n\n-- Bad: filtering happens after full scan\n... | SELECT * WHERE severity = 'ERROR'\n</code></pre> <p>Glob patterns: Use specific paths when possible:</p> <pre><code>-- Good: scans one service\nFROM read_parquet('s3://bucket/logs/my-service/**/*.parquet')\n\n-- Less efficient: scans all services\nFROM read_parquet('s3://bucket/logs/**/*.parquet')\nWHERE ServiceName = 'my-service'\n</code></pre>"},{"location":"reference/","title":"Reference","text":"<p>Configuration, environment variables, and schema definitions.</p>"},{"location":"reference/#environment-variables","title":"Environment Variables","text":"<p>All variables use the <code>OTLP2PARQUET_</code> prefix and override config file values.</p>"},{"location":"reference/#storage","title":"Storage","text":"Variable Default Description <code>OTLP2PARQUET_STORAGE_BACKEND</code> Auto Storage type: <code>s3</code> or <code>fs</code> <code>OTLP2PARQUET_S3_BUCKET</code> - S3 bucket name <code>OTLP2PARQUET_S3_REGION</code> - Storage region <code>OTLP2PARQUET_S3_ENDPOINT</code> Auto Custom S3 endpoint for MinIO or other S3-compatible storage <code>OTLP2PARQUET_STORAGE_PATH</code> <code>./data</code> Filesystem storage path"},{"location":"reference/#server","title":"Server","text":"Variable Default Description <code>OTLP2PARQUET_LISTEN_ADDR</code> <code>0.0.0.0:4318</code> HTTP listen address <code>OTLP2PARQUET_LOG_LEVEL</code> <code>info</code> Log level: <code>trace</code>, <code>debug</code>, <code>info</code>, <code>warn</code>, <code>error</code> <code>OTLP2PARQUET_LOG_FORMAT</code> <code>text</code> Log format: <code>text</code> or <code>json</code> <code>OTLP2PARQUET_MAX_PAYLOAD_BYTES</code> <code>8388608</code> Max request size (8MB)"},{"location":"reference/#batching","title":"Batching","text":"Variable Default Description <code>OTLP2PARQUET_BATCHING_ENABLED</code> <code>true</code> Enable in-memory batching <code>OTLP2PARQUET_BATCH_MAX_ROWS</code> <code>200000</code> Max rows per batch <code>OTLP2PARQUET_BATCH_MAX_BYTES</code> <code>134217728</code> Max bytes per batch (128MB) <code>OTLP2PARQUET_BATCH_MAX_AGE_SECS</code> <code>10</code> Max batch age in seconds"},{"location":"reference/#schema","title":"Schema","text":"<p>All schemas use PascalCase column names for ClickHouse compatibility.</p>"},{"location":"reference/#logs","title":"Logs","text":"Field Type Description <code>Timestamp</code> <code>Timestamp(\u03bcs)</code> Event time <code>TimestampTime</code> <code>Timestamp(\u03bcs)</code> Timestamp rounded to second <code>ObservedTimestamp</code> <code>Timestamp(\u03bcs)</code> Observer time <code>TraceId</code> <code>Binary</code> W3C trace ID (16 bytes) <code>SpanId</code> <code>Binary</code> W3C span ID (8 bytes) <code>TraceFlags</code> <code>UInt32</code> W3C trace flags <code>SeverityText</code> <code>String</code> Severity level (<code>INFO</code>, <code>WARN</code>, <code>ERROR</code>) <code>SeverityNumber</code> <code>Int32</code> Numeric severity (0-24) <code>Body</code> <code>String</code> Log message (JSON-encoded) <code>ServiceName</code> <code>String</code> Extracted from <code>service.name</code> <code>ServiceNamespace</code> <code>String</code> Extracted from <code>service.namespace</code> <code>ServiceInstanceId</code> <code>String</code> Extracted from <code>service.instance.id</code> <code>ResourceAttributes</code> <code>String</code> Resource attributes (JSON-encoded) <code>ResourceSchemaUrl</code> <code>String</code> Resource schema URL <code>ScopeName</code> <code>String</code> Instrumentation scope name <code>ScopeVersion</code> <code>String</code> Instrumentation scope version <code>ScopeAttributes</code> <code>String</code> Scope attributes (JSON-encoded) <code>ScopeSchemaUrl</code> <code>String</code> Scope schema URL <code>LogAttributes</code> <code>String</code> Log attributes (JSON-encoded)"},{"location":"reference/#traces","title":"Traces","text":"Field Type Description <code>Timestamp</code> <code>Timestamp(\u03bcs)</code> Span start time <code>TraceId</code> <code>String</code> W3C trace ID <code>SpanId</code> <code>String</code> W3C span ID <code>ParentSpanId</code> <code>String</code> Parent span ID <code>TraceState</code> <code>String</code> W3C trace state <code>SpanName</code> <code>String</code> Span name <code>SpanKind</code> <code>String</code> Span kind (<code>SERVER</code>, <code>CLIENT</code>) <code>ServiceName</code> <code>String</code> Extracted from <code>service.name</code> <code>ResourceAttributes</code> <code>String</code> Resource attributes (JSON-encoded) <code>ScopeName</code> <code>String</code> Instrumentation scope name <code>ScopeVersion</code> <code>String</code> Instrumentation scope version <code>SpanAttributes</code> <code>String</code> Span attributes (JSON-encoded) <code>Duration</code> <code>Int64</code> Duration in nanoseconds <code>StatusCode</code> <code>String</code> Status code (<code>Ok</code>, <code>Error</code>, <code>Unset</code>) <code>StatusMessage</code> <code>String</code> Status message <code>EventsTimestamp</code> <code>List&lt;Timestamp&gt;</code> Event timestamps <code>EventsName</code> <code>List&lt;String&gt;</code> Event names <code>EventsAttributes</code> <code>List&lt;String&gt;</code> Event attributes (JSON-encoded) <code>LinksTraceId</code> <code>List&lt;String&gt;</code> Linked trace IDs <code>LinksSpanId</code> <code>List&lt;String&gt;</code> Linked span IDs <code>LinksTraceState</code> <code>List&lt;String&gt;</code> Linked trace states <code>LinksAttributes</code> <code>List&lt;String&gt;</code> Link attributes (JSON-encoded)"},{"location":"reference/#metrics","title":"Metrics","text":"<p>Metrics are stored in separate tables by type: <code>gauge</code>, <code>sum</code>, <code>histogram</code>, <code>exponential_histogram</code>, <code>summary</code>.</p> <p>Base fields (all metric types):</p> Field Type Description <code>Timestamp</code> <code>Timestamp(\u03bcs)</code> Data point time <code>ServiceName</code> <code>String</code> Extracted from <code>service.name</code> <code>ResourceAttributes</code> <code>String</code> Resource attributes (JSON-encoded) <code>ScopeName</code> <code>String</code> Instrumentation scope name <code>ScopeVersion</code> <code>String</code> Instrumentation scope version <code>MetricName</code> <code>String</code> Metric name <code>MetricDescription</code> <code>String</code> Metric description <code>MetricUnit</code> <code>String</code> Metric unit <code>Attributes</code> <code>String</code> Data point attributes (JSON-encoded) <p>Type-specific fields:</p> <p>Gauge:</p> Field Type Description <code>Value</code> <code>Float64</code> Observed value <p>Sum:</p> Field Type Description <code>Value</code> <code>Float64</code> Sum value <code>AggregationTemporality</code> <code>Int32</code> Delta or Cumulative <code>IsMonotonic</code> <code>Boolean</code> Monotonic flag <p>Histogram:</p> Field Type Description <code>Count</code> <code>Int64</code> Number of values <code>Sum</code> <code>Float64</code> Sum of values <code>BucketCounts</code> <code>List&lt;Int64&gt;</code> Values per bucket <code>ExplicitBounds</code> <code>List&lt;Float64&gt;</code> Bucket upper bounds <code>Min</code> <code>Float64</code> Minimum value <code>Max</code> <code>Float64</code> Maximum value <p>Exponential Histogram:</p> Field Type Description <code>Count</code> <code>Int64</code> Number of values <code>Sum</code> <code>Float64</code> Sum of values <code>Scale</code> <code>Int32</code> Histogram scale <code>ZeroCount</code> <code>Int64</code> Zero values <code>PositiveOffset</code> <code>Int32</code> Positive bucket offset <code>PositiveBucketCounts</code> <code>List&lt;Int64&gt;</code> Positive bucket counts <code>NegativeOffset</code> <code>Int32</code> Negative bucket offset <code>NegativeBucketCounts</code> <code>List&lt;Int64&gt;</code> Negative bucket counts <code>Min</code> <code>Float64</code> Minimum value <code>Max</code> <code>Float64</code> Maximum value <p>Summary:</p> Field Type Description <code>Count</code> <code>Int64</code> Number of values <code>Sum</code> <code>Float64</code> Sum of values <code>QuantileValues</code> <code>List&lt;Float64&gt;</code> Values at quantiles <code>QuantileQuantiles</code> <code>List&lt;Float64&gt;</code> Quantile points"},{"location":"reference/#file-layout","title":"File Layout","text":"<p>Parquet files use Hive-style partitioning:</p> <pre><code>logs/{service}/year={year}/month={month}/day={day}/hour={hour}/{timestamp}-{uuid}.parquet\ntraces/{service}/year={year}/month={month}/day={day}/hour={hour}/{timestamp}-{uuid}.parquet\nmetrics/{type}/{service}/year={year}/month={month}/day={day}/hour={hour}/{timestamp}-{uuid}.parquet\n</code></pre> <p>Where <code>{type}</code> is one of: <code>gauge</code>, <code>sum</code>, <code>histogram</code>, <code>exponential_histogram</code>, <code>summary</code>.</p>"},{"location":"sending-data/","title":"Send Data","text":"<p>Send OpenTelemetry data to your otlp2parquet endpoint.</p>"},{"location":"sending-data/#endpoints","title":"Endpoints","text":"Signal Endpoint Content-Type Logs <code>/v1/logs</code> <code>application/json</code> or <code>application/x-protobuf</code> Traces <code>/v1/traces</code> <code>application/json</code> or <code>application/x-protobuf</code> Metrics <code>/v1/metrics</code> <code>application/json</code> or <code>application/x-protobuf</code>"},{"location":"sending-data/#quick-test-curl","title":"Quick Test (curl)","text":"LogsTracesMetrics <pre><code>curl -X POST http://localhost:4318/v1/logs \\\n  -H \"Content-Type: application/json\" \\\n  -d @testdata/log.json\n</code></pre> <pre><code>curl -X POST http://localhost:4318/v1/traces \\\n  -H \"Content-Type: application/json\" \\\n  -d @testdata/trace.json\n</code></pre> <pre><code>curl -X POST http://localhost:4318/v1/metrics \\\n  -H \"Content-Type: application/json\" \\\n  -d @testdata/metrics_gauge.json\n</code></pre>"},{"location":"sending-data/#opentelemetry-collector-recommended","title":"OpenTelemetry Collector (Recommended)","text":"<p>For production, use the OpenTelemetry Collector to batch data before sending. This reduces costs and creates more efficient Parquet files.</p> <pre><code># otel-collector-config.yaml\nreceivers:\n  otlp:\n    protocols:\n      grpc:\n      http:\n\nprocessors:\n  batch:\n    send_batch_max_size: 10000\n    timeout: 10s\n\nexporters:\n  otlphttp:\n    endpoint: http://your-endpoint:4318/\n\nservice:\n  pipelines:\n    logs:\n      receivers: [otlp]\n      processors: [batch]\n      exporters: [otlphttp]\n    traces:\n      receivers: [otlp]\n      processors: [batch]\n      exporters: [otlphttp]\n    metrics:\n      receivers: [otlp]\n      processors: [batch]\n      exporters: [otlphttp]\n</code></pre>"},{"location":"sending-data/#sdk-configuration","title":"SDK Configuration","text":"<p>Point your OTel SDK exporter at your endpoint:</p> PythonNode.jsGo <pre><code>from opentelemetry.exporter.otlp.proto.http.log_exporter import OTLPLogExporter\n\nexporter = OTLPLogExporter(endpoint=\"http://localhost:4318/v1/logs\")\n</code></pre> <pre><code>const { OTLPLogExporter } = require('@opentelemetry/exporter-logs-otlp-http');\n\nconst exporter = new OTLPLogExporter({\n  url: 'http://localhost:4318/v1/logs',\n});\n</code></pre> <pre><code>import \"go.opentelemetry.io/otel/exporters/otlp/otlplog/otlploghttp\"\n\nexporter, _ := otlploghttp.New(ctx,\n  otlploghttp.WithEndpoint(\"localhost:4318\"),\n  otlploghttp.WithInsecure(),\n)\n</code></pre>"},{"location":"sending-data/#troubleshooting","title":"Troubleshooting","text":"Problem Solution Parse errors Ensure valid OTLP JSON/protobuf payload 413 Payload Too Large Batch smaller or increase <code>OTLP2PARQUET_MAX_PAYLOAD_BYTES</code> Connection refused Check endpoint URL and firewall rules Storage write failures Check bucket permissions and credentials"}]}